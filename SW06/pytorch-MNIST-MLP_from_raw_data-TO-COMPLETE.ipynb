{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron from MNIST raw data using PyTorch\n",
    "\n",
    "This notebook will guide you through the use of the `pytroch` framework to train a multilayer perceptron for handwritten digits classification. You are going to use the `mnist` dataset from LeCun et al. 1998\n",
    "\n",
    "We assume you are using `torch` and `torchvision` packages, if not please install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your torch version is 1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, import pytorch, get its version and check available device.\n",
    "\n",
    "import torch\n",
    "print('Your torch version is {}'.format(torch.__version__))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix # for evaluating results\n",
    "from tqdm.notebook import tqdm # for progress bars\n",
    "\n",
    "# ... import here the different pytorch libraries you need\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MNIST dataset\n",
    "Pytorch provides a set of pre-loaded datasets including MNIST. We will convert our MNIST images into tensors when loading them. There are lots of other transformations that you can do using torchvision.transforms like Reshaping, normalizing, etc. on your images but we won't need that since MNIST is a very primitive dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform = transforms.ToTensor() # let us convert mnist data to tensors\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data', # path to save mnist data \n",
    "    train = True,                         \n",
    "    transform = Transform, \n",
    "    download = True,            \n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = Transform,\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing train_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot one train_data\n",
    "img, target = train_data[0]\n",
    "print('Size:', img.size(), 'Label:', target)\n",
    "plt.imshow(img.reshape((28,28)), cmap='gray') # to fix the TypeError: Invalid shape (1, 28, 28) for image data\n",
    "plt.title('%i' % target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple train_data\n",
    "def show_imgs(X):\n",
    "    figure = plt.figure(figsize=(10, 8))\n",
    "    cols, rows = 5, 5\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(X), size=(1,)).item()\n",
    "        img, label = X[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "show_imgs(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-layer network and weight visualisation\n",
    "## Building the Network\n",
    "\n",
    "- The code is straightforward. In Pytorch there isn't any implementation for the input layer, the input is passed directly into the first hidden layer. However, you'll find the InputLayer in the Keras implementation.\n",
    "\n",
    "- The number of neurons in the hidden layers and the number of hidden layers is a parameter that can be played with, to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        return self.fc1(x)\n",
    "\n",
    "model = Net().to(device)\n",
    "# ... this model uses the crossentropy loss\n",
    "criterion = \n",
    "# and the RMSprop as optimizer \n",
    "optimizer =   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# ... print model infomration with summary() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the Model\n",
    "\n",
    "Our model is now ready to be trained. Let's define functions to train() and validate() the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    \n",
    "    # ... Set model to training mode\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # ...  Reset the gradient buffers to zero     \n",
    "        \n",
    "        output = # ... Pass data through the network\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        # Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        # Applies the calcualted gradients to each parameter\n",
    "        # based on the selected optimizer and its configuration.\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # The maximum value of dimension 1 (class dimension)\n",
    "        # is the predicted class. Only the class index is\n",
    "        # relevant, hence the first return value is ignored.\n",
    "        # as: values, indices = torch.max(output, dim=1)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        \n",
    "        # pred == target gives a boolean tensor, where each\n",
    "        # element represents whether it was correctly predicted.\n",
    "        # Taking the sum gives the number of correct predictions\n",
    "        # for this particular batch.\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    \n",
    "    # Divided by len(data_loader.dataset) because it's the number\n",
    "    # of correct predictions in total.\n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    # Divided by len(data_loader) because it is the sum across\n",
    "    # all batches, therefore it's divided by the number of batches.\n",
    "    # There is a difference in the length of the data_loader compared\n",
    "    # to the underlying dataset, which the data_loader partitions into\n",
    "    # batches:\n",
    "    #   - len(data_loader) == number of batches the dataloader can create\n",
    "    #   - len(data_loader.dataset) == number of samples in the dataset\n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.inference_mode() disables certain features that are not required\n",
    "# for inference, such as autograd and other tracking.\n",
    "# It's optional, but improves performance.\n",
    "# Alternative: @torch.no_grad(), which only disables autograd. \n",
    "@torch.inference_mode()\n",
    "\n",
    "def validate(model, data_loader):\n",
    "   \n",
    "    # ...  put the model in eval mode, which disables raining specific behaviour, such as Dropout.\n",
    "    \n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    \n",
    "    for data, target in tqdm(data_loader, desc=\"Validation\"):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "        # Keep the predictions and targets to inspect later on.\n",
    "        # .detach() decouples it from the computational graph\n",
    "        # and .cpu() ensures it's on the CPU, since it shouldn't\n",
    "        # occupy any unnecesary GPU memory.\n",
    "        total_preds.append(pred.detach().cpu())\n",
    "        # target isn't tracked in the computational graph\n",
    "        # (it's a leaf with requires_grad=False), adding the\n",
    "        # .detach() doesn't do anything, but if in doubt, just add it.\n",
    "        total_targets.append(target.cpu())\n",
    "\n",
    "    val_loss /= len(data_loader)\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, total_correct, len(data_loader.dataset), 100 * accuracy))\n",
    "    \n",
    "    return {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"predictions\": torch.cat(total_preds),\n",
    "        \"targets\": torch.cat(total_targets),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the train() function. An epoch means one pass through the whole training data. After each epoch, we evaluate the model using validate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, model, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    \n",
    "    val_result = validate(model, test_loader)\n",
    "    val_losses.append(val_result[\"loss\"])\n",
    "    val_accuracies.append(val_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Loss and Accuracy evaluation during training\n",
    "\n",
    "Let's now visualize how the training progressed.\n",
    "\n",
    "Loss is a function of the difference of the network output and the target values. We are minimizing the loss function during training so it should decrease over time.\n",
    "Accuracy is the classification accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "ax1.plot(train_losses, label='Training loss')\n",
    "ax1.plot(val_losses, label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(train_accuracies, label='Training acc')\n",
    "ax2.plot(val_accuracies, label='Test acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_result is from the validation of the last epoch above.\n",
    "# But you could run it specifically with:\n",
    "#\n",
    "# val_result = validate(model, test_loader)\n",
    "\n",
    "confusion_matrix(val_result[\"predictions\"], val_result[\"targets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weights\n",
    "The weights connected to a given neuron, when using a one-layer network, have the same shape as the input. They can therefore be plot. To do so we need to re-scale the weight values into 0-255 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight of the classification layer\n",
    "classifier_weight = model.fc1.weight.detach().cpu()\n",
    "\n",
    "print(f\"Weight of classification layer has size: {classifier_weight.size()}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "# Plot the weight as an image for each class\n",
    "# because the size is: [num_classes x num_inputs],\n",
    "# it can be iterated over the first dimension to\n",
    "# get the weight for each individual class.\n",
    "\n",
    "for i, weight in enumerate(classifier_weight):\n",
    "    ax = fig.add_subplot(1, 10, 1+i)\n",
    "    # now put back the pixel values to 0-255 doing a min-max norm and multiplying by 255\n",
    "    min_value = #...\n",
    "    max_value = #...\n",
    "    im = #...\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im.reshape(28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
