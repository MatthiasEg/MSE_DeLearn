{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d23342",
   "metadata": {},
   "source": [
    "# PW05 - Group 4\n",
    "* Florian BÃ¤r\n",
    "* Matthias Egli\n",
    "* Manuel Vogel\n",
    "* Adrian Willi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77579401-4d7c-4c1e-9771-6c223bd3248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68525666-7794-4f06-96c5-917826451ac5",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "The test function that is used for illustrating the workings of the optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ca827-5bc1-494e-a539-3b1077a7c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(x):\n",
    "    return x[0]**2/15.0+x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02224a6-102d-40f6-99b6-a311a292b909",
   "metadata": {},
   "source": [
    "Utility for plotting the contours of a given function`f` in a range `r` (4-tuple) and 20 levels up to a max level given by `s`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ce574-9e58-4dc2-a3dd-a778fc383c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(r, s, f):\n",
    "    matplotlib.rcParams['xtick.direction'] = 'out'\n",
    "    matplotlib.rcParams['ytick.direction'] = 'out'\n",
    "    delta = 0.01\n",
    "    X = np.arange(r[0], r[1], delta)\n",
    "    Y = np.arange(r[2], r[3], delta)\n",
    "    X,Y = np.meshgrid(X, Y)  \n",
    "    XX = np.stack((X,Y), axis=0)\n",
    "    Z=f(XX)\n",
    "    V=np.arange(0.2,s,s/20)**2\n",
    "    #CS = plt.contour(X, Y, Z, V, colors=colors)\n",
    "    #plt.clabel(CS, inline=1, fontsize=10)\n",
    "    CS = plt.contour(X, Y, Z, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d654822-8721-466d-817b-4e01cf48b397",
   "metadata": {},
   "source": [
    "Optimisation (i.e. minimisation) of the function `f` starting at `x` (pytorch tensor), by using the given `optimizer` over`nepochs`epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4807cd-93dd-48da-8a11-20ced3ae56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(f, x, nepochs, optimizer):\n",
    "    params = [\n",
    "        x,\n",
    "    ]\n",
    "    xhist = [x[0].item()]\n",
    "    yhist = [x[1].item()]\n",
    "    optimizer.param_groups[0][\"params\"] = params\n",
    "\n",
    "    epochs_to_optima = 0\n",
    "    for i in range(nepochs):\n",
    "        y = f(x)\n",
    "        optimizer.zero_grad()\n",
    "        y.backward()\n",
    "        optimizer.step()\n",
    "        if (\n",
    "            np.sqrt(x[0].item() ** 2 + x[1].item() ** 2) <= 0.001\n",
    "            and epochs_to_optima != 1\n",
    "        ):\n",
    "            print(f\"Took {str(i + 1)} steps to the optima: \", end=\"\")\n",
    "            epochs_to_optima += 1\n",
    "\n",
    "        xhist.append(x[0].item())\n",
    "        yhist.append(x[1].item())\n",
    "    return xhist, yhist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47af19f-e199-4a7f-93e0-26ed0681afba",
   "metadata": {},
   "source": [
    "RMS error for measuring the discrepancy to the final target (for the test function above it is (0,0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9fa59-4df0-467e-8a50-996a9995e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(xhist,yhist):\n",
    "    x,y = xhist[-1], yhist[-1]\n",
    "    return \"%6.3f, (%6.3f, %6.3f)\"%(np.sqrt(x**2+y**2),x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f5b22-3c4a-445f-832b-e93fd0c0933a",
   "metadata": {},
   "source": [
    "### Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84de4bf-bea3-4db7-b483-d2f66cc7c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 200\n",
    "\n",
    "plt.figure(1,figsize=(12,6))\n",
    "plot_contours([-3.5,0.,-4.,4.], 4., test_function)\n",
    "\n",
    "# initial value\n",
    "x = torch.tensor([-3.0,-4.0]).requires_grad_()\n",
    "\n",
    "optimizer = torch.optim.SGD([x],lr=.3)\n",
    "xhist, yhist = optimize(test_function, x, nepochs, optimizer)  \n",
    "\n",
    "plt.plot(xhist,yhist,\"b-+\", label=\"lr=0.3\")\n",
    "print(\"SGD - lr=0.3 \", error(xhist,yhist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0aa86-04e0-4df0-a310-2e1647dcf746",
   "metadata": {},
   "source": [
    "### Tasks \n",
    "\n",
    "For better comparison, always use the same (non-optimal) initial point.  \n",
    "\n",
    "* Study SGD for different learning rates.\n",
    "* Compare Momentum for different momentum parameters and characterise how the optimisation paths qualitatively change.\n",
    "* Can you spot a difference with / without Nesterov? \n",
    "* Play with different settings for the parameter $\\beta_1$ and characterise how the optimisation paths qualitatively change.\n",
    "* Finally, check different settings for Adam. Experiment with different learning rates. Can you experimentally verify that Adam is less sensitive to the choice of the learning rate than e.g. SGD?\n",
    "* Identify your best parameters for each of the models above - for the given example. How many steps are needed with these settings to reach the minimum of the function at (0,0) within an accuracy of 1.0e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac95edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 20\n",
    "\n",
    "# initial value\n",
    "x = torch.tensor([-3.0,-4.0]).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6541308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def print_learning(\n",
    "    optimizer_name: str,\n",
    "    lrs: List[float] = [0.001],\n",
    "    momentums: List[float] = [0],\n",
    "    nesterov: bool = False,\n",
    "    b1s: List[float] = [0.9],\n",
    "):\n",
    "    plt.figure(1, figsize=(12, 6))\n",
    "    plot_contours([-3.5, 0.0, -4.0, 4.0], 4.0, test_function)\n",
    "\n",
    "    for lr, momentum in product(lrs, momentums):\n",
    "        for b1 in b1s:\n",
    "            nepochs = 200\n",
    "\n",
    "            plt.figure(1, figsize=(12, 6))\n",
    "            plot_contours([-3.5, 0.0, -4.0, 4.0], 4.0, test_function)\n",
    "\n",
    "            # initial value\n",
    "            x = torch.tensor([-3.0, -4.0]).requires_grad_()\n",
    "\n",
    "            if optimizer_name == \"SGD\":\n",
    "                optimizer = torch.optim.SGD(\n",
    "                    [x], lr=lr, momentum=momentum, nesterov=nesterov\n",
    "                )\n",
    "            elif optimizer_name == \"Adam\":\n",
    "                optimizer = torch.optim.Adam([x], lr=lr, betas=(b1, 0.999))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            xhist, yhist = optimize(test_function, x, nepochs, optimizer)\n",
    "\n",
    "            plt.plot(\n",
    "                xhist,\n",
    "                yhist,\n",
    "                \"-+\",\n",
    "                label=f\"lr={lr}, momentum={momentum}, nesterov={nesterov}, b1={b1}\",\n",
    "                color=(np.random.random(), np.random.random(), np.random.random()),\n",
    "            )\n",
    "            plt.legend(loc=\"best\")\n",
    "            print(f\"{optimizer_name} - lr={lr} - momentum={momentum}, nesterov={nesterov}, b1={b1}\", error(xhist, yhist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25810-aa83-4488-835f-5b2a49fe0191",
   "metadata": {},
   "source": [
    "#### 1. SDG for different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_learning(optimizer_name=\"SGD\", lrs=[0.001, 0.01, 0.1, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25810-aa83-4488-835f-5b2a49fe0191",
   "metadata": {},
   "source": [
    "#### 2. Compare Momentum for different momentum parameters and characterise how the optimisation paths qualitatively change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2daa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_learning(optimizer_name=\"SGD\", lrs=[0.001, 0.01, 0.1], momentums=np.linspace(0.1, 0.9, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25810-aa83-4488-835f-5b2a49fe0191",
   "metadata": {},
   "source": [
    "#### 3. Can you spot a difference with / without Nesterov? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_learning(optimizer_name=\"SGD\", lrs=[0.001, 0.01, 0.1], momentums=np.linspace(0.1, 0.9, 5), nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25810-aa83-4488-835f-5b2a49fe0191",
   "metadata": {},
   "source": [
    "#### 4. Play with different settings for the parameter $\\beta_1$ and characterise how the optimisation paths qualitatively change.\n",
    "#### 5. Finally, check different settings for Adam. Experiment with different learning rates. Can you experimentally verify that Adam is less sensitive to the choice of the learning rate than e.g. SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_learning(optimizer_name=\"Adam\", lrs=np.linspace(0.01, 1, 5), b1s=np.linspace(0.01, 0.9, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7a056",
   "metadata": {},
   "source": [
    "#### 6. Identify your best parameters for each of the models above - for the given example. How many steps are needed with these settings to reach the minimum of the function at (0,0) within an accuracy of 1.0e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b291d",
   "metadata": {},
   "source": [
    "See printout for the above configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
