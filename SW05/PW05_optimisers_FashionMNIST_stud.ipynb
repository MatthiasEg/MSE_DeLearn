{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.mnist.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.mnist.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2da4ed-af9a-4fc7-b59f-135b51241d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b30c81-d89f-4947-aa09-ab8c70dd262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data),len(validation_data),len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61b15df-a84b-483d-9365-d8f66390b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(units = [28*28,250,80,10]):\n",
    "    seq = [torch.nn.Flatten()]\n",
    "    for i in range(len(units)-2):\n",
    "        seq.append(torch.nn.Linear(units[i],units[i+1]))\n",
    "        seq.append(torch.nn.Sigmoid())\n",
    "    seq.append(torch.nn.Linear(units[-2],units[-1]))\n",
    "    return torch.nn.Sequential(*seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 250]         196,250\n",
      "           Sigmoid-3                  [-1, 250]               0\n",
      "            Linear-4                   [-1, 80]          20,080\n",
      "           Sigmoid-5                   [-1, 80]               0\n",
      "            Linear-6                   [-1, 10]             810\n",
      "================================================================\n",
      "Total params: 217,140\n",
      "Trainable params: 217,140\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.83\n",
      "Estimated Total Size (MB): 0.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = mlp()\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa393a-9f99-48f4-996d-cf8d2a3b8cd5",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Implement the training / evaluation loop\n",
    "\n",
    "Remember training / validation cost and accuracy per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_eval(model, optimizer, nepochs, training_loader, test_loader, scheduler=None):\n",
    "    cost_hist = []\n",
    "    cost_hist_test = []\n",
    "    acc_hist = []\n",
    "    acc_hist_test = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        size = len(training_loader.dataset)\n",
    "        nbatches = len(training_loader)\n",
    "        size_test = len(test_loader.dataset)\n",
    "        nbatches_test = len(test_loader)\n",
    "        cost, acc = 0.0, 0.0\n",
    "        for batch, (X, Y) in enumerate(training_loader):\n",
    "            pred = model(X)\n",
    "            loss = cost_ce(pred, Y)\n",
    "            cost += loss.item()\n",
    "            acc += (pred.argmax(dim=1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "            # gradient, parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        cost /= nbatches\n",
    "        acc /= size\n",
    "    \n",
    "        model.eval()\n",
    "        cost_test, acc_test = 0.0, 0.0        \n",
    "        with torch.no_grad():\n",
    "            for X, Y in test_loader:\n",
    "                pred = model(X)\n",
    "                cost_test += cost_ce(pred, Y).item()\n",
    "                acc_test += (pred.argmax(dim=1) == Y).type(torch.float).sum().item()\n",
    "        cost_test /= nbatches_test\n",
    "        acc_test /= size_test\n",
    "        print(\"Epoch %i: %f, %f, %f, %f\"%(epoch, cost, acc, cost_test, acc_test))\n",
    "        cost_hist.append(cost)\n",
    "        cost_hist_test.append(cost_test)\n",
    "        acc_hist.append(acc)\n",
    "        acc_hist_test.append(acc_test)\n",
    "    return cost_hist, cost_hist_test, acc_hist, acc_hist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a52b-a332-4844-8d7d-6998893a2d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse Different Optimisers with different Settings \n",
    "\n",
    "Use the code above to explore different settings for the different optimizers. Use batchsize 64.\n",
    "\n",
    "1. SGD: Revisit plain SGD (without momentum) and try different learning rates (lr). Create suitable comparison plots (learning curves with the train and validate cost and accuracy) for (~3) different learning rates. Describe in words what you see. What is a reasonable number of epochs (nepochs)? What is your recommended best choice (lr, nepochs) for the given problem?\n",
    "\n",
    "2. Momentum: Play with different settings when using momentum: learning rate, momentum parameter, Nesterov flag. Start with momentum=0.9 without Nesterov and suitable learning rate, then vary the momentum parameter and independently the learning rate. Can you see an impact of using Nesterov? What is your recommended best choice (lr, momentum, nesterov, nepochs) for the given problem?\n",
    "\n",
    "3. RMSprop: Same thing now for RMSprop (without momentum). Play with different settings when using RMSprop: lr, alpha. Start with the default settings of pytorch with (lr=0.01, alpha=0.99,centered=False). Then vary alpha and independently the learning rate. Can you see an impact when using centered=True? What is your recommended best choice (learning rate, alpha, centered, nepochs) for the given problem?\n",
    "\n",
    "4. Adam: Same thing now for Adam. Play with different settings. Start with the default settings of pytorch. What is your recommended best choice for the given problem?\n",
    "\n",
    "5. Learning rate schedule: Implement a learning rate schedule for SGD (without momentum) - by using e.g. StepLR. What are your preferred settings for the given task? Note the way how the scheduler is incorporated into the `train_eval`-method above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa1c2b-50d7-4b23-91c8-c47eec5fc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 64\n",
    "nepochs = \n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=nbatch, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=nbatch, shuffle=True)\n",
    "    \n",
    "model = mlp()\n",
    "optimizer = ...\n",
    "cost_hist, cost_hist_test, acc_hist, acc_hist_test = train_eval(model, optimizer, nepochs, training_loader, validation_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f24c35-9def-48ba-a505-20b99d450584",
   "metadata": {},
   "source": [
    "### Plots and Comments (for the different steps described above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ab6cb-7787-4f18-8763-ae231c610c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
