{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3c3737-73dc-481a-8cf0-afb3657ca3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de7e72-0814-405b-8c2c-cefa78d66dc0",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "A Model here consists of a succession of layers.\n",
    "\n",
    "Each layer is implemented as a class with the following API-methods:\n",
    "\n",
    "`forward(torch.tensor)`: Computes the forward pass through the layer, i.e. $x\\rightarrow a$<br> and keeps the information needed for computing the backward pass as member variables. \n",
    "`backward(torch.tensor)`: Computes the backward pass through the layer in form of the derivatives, i.e. $da \\rightarrow dx$. On the fly, it also computes the derivatives w.r.t. the parameters of the layer and keeps them as member variables. It assumes that `forward` method has been run before. <br>\n",
    "`update(lr)`: Updates the parameters of the layer in accordance with vanilla gradient descent and scalar learning rate `lr`. It assumes that the `forward` and the `backward`-pass has been run before.  \n",
    "\n",
    "The tensors defined as inputs to the `forward`/`backward`-method are two dimensional with the sample index in the first and the the feature index in the second dimension. \n",
    "\n",
    "For fully connected layers with activation function $s(\\cdot)$ the formulas are given as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdbdc5-d75a-416b-a5df-dfd3ef1fca05",
   "metadata": {},
   "source": [
    "__Forward path:__\n",
    "\n",
    "$X_{i,j}$: Tensor with shape $(n_b,n_x)$ where $n_b$ is the number of samples in the batch and $n_x$ the number of input features (for MNIST: 784).\n",
    "\n",
    "$Z_{i,j} = \\sum_k X_{i,k} W_{j,k} \\qquad (Z = X \\cdot W^T + b)$ $\\qquad$ ($W$ a tensor of shape $(n_h,n_x)$)\n",
    "\n",
    "$A_{i,j} = s(Z_{i,j}) \\qquad\\qquad (A = s(Z))$\n",
    "\n",
    "__Backward path:__ (with $n_b$ the number of samples in a batch)\n",
    "\n",
    "$dx_{i,k} = \\frac{\\partial L}{\\partial x_{i,k}} = \\sum_j \\frac{\\partial L}{\\partial a_{i,j}} \\frac{\\partial a_{i,j}}{\\partial x_{i,k}} = \\sum_j da_{i,j} s^\\prime(z_{i,j})\\cdot \\frac{\\partial z_{i,j}}{\\partial x_{i,k}} = \\sum_j da_{i,j} s^\\prime(z_{i,j}) W_{j,k}$<br>\n",
    "\n",
    "$dW_{k,j} = \\frac{\\partial L}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_i da_{i,k}\\frac{\\partial a_{i,k}}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_j da_{i,k} s^\\prime(z_{i,k}) \\frac{\\partial z_{i,k}}{\\partial W_{k,j}} = \\frac{1}{n_b}\\sum_j da_{i,k} s^\\prime(z_{i,k}) x_{i,j}$<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad958e2-bb1e-493f-9c4d-40514e20c479",
   "metadata": {},
   "source": [
    "__Parameter Initialisation__ \n",
    "\n",
    "The parameters need to be initialised which will be a topic later in the course. For now use the following rules: \n",
    "* weights normally distributed with mean $0$ and stdev $1/\\sqrt{n_h}$\n",
    "* bias initialized with zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d7916-88e9-4cc9-91e8-ec600741370d",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Important Note on the Implementation</span>\n",
    "\n",
    "Make sure that all the tensors used anywhere in the model components below have `requires_grad=False`.\n",
    "Autograd functionality is not allowed for computing the gradients. - Autograd will be used below for testing whether your implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae0f20-6928-481d-96b2-02cbfa298760",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c81dc286-9543-4278-a174-6fbba6be026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, nx, nh):\n",
    "        \"\"\"\n",
    "        nx -- number of input features, i.e. shape of input tensors x given by (*,nb_input)\n",
    "        nh -- number of output features, i.e. shape of output tensor z given by (*,nb_hidden)\n",
    "        \"\"\"    \n",
    "        self.nx = nx\n",
    "        self.nh = nh\n",
    "        self.w = torch.empty(nh, nx).normal_(0, 1./math.sqrt(self.nh))\n",
    "        self.b = torch.zeros(nh)\n",
    "        self.dw = torch.zeros_like(self.w)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "        self.x = None\n",
    "        self.dx = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass through the layer\n",
    "        x -- input tensor\n",
    "        returns z \n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        self.x = x\n",
    "        return x@self.w.T + self.b\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        \"\"\"\n",
    "        Computes the backward pass through the layer incl. the derivatives w.r.t. input x (dx), weight w (dw) and bias b (db).\n",
    "        dz -- tensor with the backprop'd error signal with the same shape as z.         \n",
    "        returns dx\n",
    "        \"\"\"\n",
    "        assert len(dz.shape)==2 and dz.shape[1] == self.nh\n",
    "        ### YOUR CODE START ###\n",
    "        self.db = dz\n",
    "        self.dw = dz.T@self.x\n",
    "        self.dx = dz@self.w\n",
    "\n",
    "        return self.dx\n",
    "        ### YOUR CODE END ###\n",
    "            \n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the model (weights w and bias b) with the gradient w.r.t. w and b and learning rate.\n",
    "        returns None\n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "        ### YOUR CODE END ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9fd5b-a7b6-4a74-bd71-ab9bc1c79263",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">SHAPE TEST:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d46297-caf7-4b69-bc8f-9b32eb25590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearLayer(3,4)\n",
    "assert (4,3) == linear.w.shape\n",
    "assert (4,)  == linear.b.shape\n",
    "\n",
    "x = torch.tensor([[1.,2,3],[4,5,6]])\n",
    "a = linear.forward(x)\n",
    "assert (2,4) == a.shape\n",
    "\n",
    "dz = torch.tensor([[1.,1,1,1],[2.,2,2,2]])\n",
    "dx = linear.backward(dz)\n",
    "assert (2,3) == dx.shape\n",
    "assert (4,3) == linear.dw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7383-adf3-4822-9d99-644b9eb33e07",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "__Sigmoid__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae86ae0a-0382-4349-acb9-6b1e29be36ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SigmoidActivation():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, z):\n",
    "        ### YOUR CODE START ###\n",
    "        self.z = z\n",
    "        a = 1 / 1 + torch.exp(-z)\n",
    "        return a\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "    def backward(self, da):\n",
    "        ### YOUR CODE START ###\n",
    "        dz = da * self.z\n",
    "        return dz\n",
    "        ### YOUR CODE END ###\n",
    "            \n",
    "    def update(self, lr):\n",
    "        ### YOUR CODE START ###\n",
    "        self.z -= lr * self.z\n",
    "        ### YOUR CODE END ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d949cda-b92b-4c17-a868-73dec9fd1718",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now implement an MLP as a succession of layers - linear layers and non-linear activation layers.\n",
    "For creating an instance, you will pass the following arguments: \n",
    "* nx: number of input features\n",
    "* nunits: list of number of units in the hidden layers including the output layer\n",
    "\n",
    "Add a list of layers as member variable.\n",
    "\n",
    "Use just a linear layer at the end. Further below we will use a CE loss which is based on the finally output logit values (see lecture of week 2) where the softmax probabilities are implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee66c47a-d587-47af-8d10-384a157948f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    def __init__(self, nx, nunits):\n",
    "        self.nx = nx\n",
    "        self.nlayers = len(nunits)\n",
    "        self.nunits = nunits\n",
    "        self.nunits.insert(0,nx)\n",
    "        self.nclasses = self.nunits[-1]\n",
    "        self.layers = []\n",
    "        \n",
    "        ### YOUR CODE START ###\n",
    "        # instantiate the different layers (linear and activations)\n",
    "        for current_layer, units_of_layer in enumerate(nunits):\n",
    "            self.layers.append(LinearLayer(nunits[current_layer-1] if current_layer > 0 else nx, units_of_layer))\n",
    "            if not current_layer == self.nlayers:\n",
    "                self.layers.append(SigmoidActivation())\n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x - input tensor        \n",
    "        returns output tensor of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE START ###\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        \n",
    "        return a\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        dy - derivative w.r.t. output tensor\n",
    "        \n",
    "        returns derivative with respect to the input tensor of the model; \n",
    "        on the fly compute all the derivatives w.r.t. parameters of the model\n",
    "        \"\"\"        \n",
    "        ### YOUR CODE START ###\n",
    "        dz = dy\n",
    "        print(f'dy shape: {dy.shape}')\n",
    "        for layer in self.layers[::-1]:\n",
    "            dz = layer.backward(dz)\n",
    "        \n",
    "        return dz\n",
    "        ### YOUR CODE END ###\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update the parameters with the given (stored) derivatives w.r.t. model parameters by using the given learning rate. \n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "        ### YOUR CODE END ###\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0859db-1d16-4652-8549-b09fa695602b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">SHAPE TEST:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6020ece6-ccfe-4242-b72c-4d00c0c1f07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy shape: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "nx = 2\n",
    "nunits = [3,4]\n",
    "mlp = MLP(nx,nunits)\n",
    "# assert 3 == len(mlp.layers)\n",
    "\n",
    "x = torch.tensor([[1.,2],[3,4]])\n",
    "a = mlp.forward(x)\n",
    "assert (2,4) == a.shape\n",
    "\n",
    "da = torch.tensor([[1.,1,1,1],[2.,2,2,2]])\n",
    "dx = mlp.backward(da)\n",
    "assert (2,2) == dx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f5f56-97d9-47c3-b0c8-ee3699c2eaca",
   "metadata": {},
   "source": [
    "### Regression Test\n",
    "\n",
    "Create a regression testing that allows you to test your implementation by regressing against the gradients computed by pytorch's autograd.\n",
    "\n",
    "Below you find two functions that may be helpful in \n",
    "1. creating a reference model from the given model - makes sure that in the reference model the exact same initialized parameters are used; furthermore, that teh parameters of the linear layers (w,b) are specified as tensors with `requires_grad=True`. \n",
    "2. comparing the derivatives w.r.t. parameters for model and refmodel. It assumes that for both, model and refmodel, backprop has been executed. For the model, it means that `backward()`has been executed - for the ref model, only `forward` has been executed, but `backward` applied to the output tensor of the refmodel. For the remodel, we use `grad` of the weights and bias tensors, for the model the parameters `dw` and `db` as basis for the comparison.\n",
    "\n",
    "<span style=\"color:red\">Adjust these methods to make them compliant with your model - it uses internals of our implementation.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e33f3c4-ab4a-4271-9d5f-39cc9f53ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_refmodel(model):\n",
    "    refmodel = MLP(model.nx, model.nunits[1:])\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if isinstance(layer, LinearLayer):\n",
    "            refmodel.layers[i].w = model.layers[i].w.detach().clone()\n",
    "            refmodel.layers[i].w.requires_grad_()\n",
    "            refmodel.layers[i].b = model.layers[i].b.detach().clone()\n",
    "            refmodel.layers[i].b.requires_grad_()\n",
    "    return refmodel\n",
    "\n",
    "def test_params(model, refmodel, digits=8):\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if isinstance(layer, LinearLayer):\n",
    "            try:\n",
    "                xxref = refmodel.layers[i].w.grad.detach().numpy()\n",
    "                xx = model.layers[i].dw.numpy()\n",
    "                np.testing.assert_array_almost_equal(xx, xxref, decimal=digits, err_msg=\"Error: layer %i\"%i)\n",
    "                xxref = refmodel.layers[i].b.grad.detach().numpy()\n",
    "                xx = model.layers[i].db.numpy()\n",
    "                np.testing.assert_array_almost_equal(xx, xxref, decimal=digits, err_msg=\"Error: layer %i\"%i)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"test failed - reason:\",e) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf6929-9a82-4ec6-94d1-90ccad1752e0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> REGRESSION TEST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f80d2e20-4fb6-4ed9-ab7f-670d9aca8b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy shape: torch.Size([2, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 1x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matthias\\Documents\\MSE\\S2_code\\MSE_DeLearn\\SW03\\pw03_backprop_stud.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000018?line=9'>10</a>\u001b[0m z \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mforward(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000018?line=10'>11</a>\u001b[0m dz \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000018?line=11'>12</a>\u001b[0m dx \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39;49mbackward(dz)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000018?line=13'>14</a>\u001b[0m \u001b[39m# create ref model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000018?line=14'>15</a>\u001b[0m mlpref \u001b[39m=\u001b[39m create_refmodel(mlp)\n",
      "\u001b[1;32mc:\\Users\\Matthias\\Documents\\MSE\\S2_code\\MSE_DeLearn\\SW03\\pw03_backprop_stud.ipynb Cell 13'\u001b[0m in \u001b[0;36mMLP.backward\u001b[1;34m(self, dy)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000012?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdy shape: \u001b[39m\u001b[39m{\u001b[39;00mdy\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000012?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000012?line=44'>45</a>\u001b[0m     dz \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(dz)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000012?line=46'>47</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dz\n",
      "\u001b[1;32mc:\\Users\\Matthias\\Documents\\MSE\\S2_code\\MSE_DeLearn\\SW03\\pw03_backprop_stud.ipynb Cell 7'\u001b[0m in \u001b[0;36mLinearLayer.backward\u001b[1;34m(self, dz)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000006?line=33'>34</a>\u001b[0m \u001b[39m### YOUR CODE START ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000006?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb \u001b[39m=\u001b[39m dz\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000006?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdw \u001b[39m=\u001b[39m dz\u001b[39m.\u001b[39;49mT\u001b[39m@self\u001b[39;49m\u001b[39m.\u001b[39;49mx\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000006?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdx \u001b[39m=\u001b[39m dz\u001b[39m@self\u001b[39m\u001b[39m.\u001b[39mw\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthias/Documents/MSE/S2_code/MSE_DeLearn/SW03/pw03_backprop_stud.ipynb#ch0000006?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdx\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x40)"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "nx = 10\n",
    "x = torch.randn(nx).reshape(-1,nx)\n",
    "\n",
    "# model instance\n",
    "nunits = [20,40,1]\n",
    "mlp = MLP(nx,nunits)\n",
    "\n",
    "# forward and backward pass\n",
    "z = mlp.forward(x)\n",
    "dz = torch.tensor([1.,1.]).reshape(-1,1)\n",
    "dx = mlp.backward(dz)\n",
    "\n",
    "# create ref model\n",
    "mlpref = create_refmodel(mlp)\n",
    "\n",
    "# only use the forward method of the ref model - and apply backward to the output tensor.\n",
    "zref = mlpref.forward(x) \n",
    "zref.backward()\n",
    "\n",
    "# compare the derivatives computed by your model with the grad computed by pytorch's autograd\n",
    "test_params(mlp, mlpref, digits=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d6187-bbbd-4bf1-ae3f-929342eeb39d",
   "metadata": {},
   "source": [
    "### Cost \n",
    "\n",
    "Use the cross-entropy cost function directly defined on the basis of the logits - which implicitly includes a softmax calculation (see lecture notes of week 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dbc1b-697b-46cd-94de-d78e701921f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    \n",
    "    def value(self, z, y):\n",
    "        \"\"\"\n",
    "        z -- tensor of shape (number of samples, number of classes) with the final logits of the model. \n",
    "        y -- tensor of shape (number of samples) with the label values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE START ###\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "    def derivative(self, z, y):\n",
    "        ### YOUR CODE START ###\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### YOUR CODE END ###\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e731f-7163-4cc9-a5b0-5c2a000ef8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CELoss()\n",
    "ypred = torch.log(torch.tensor([[0.5,0.4,0.1],[0.2,0.1,0.7]])).reshape(-1,3) # -> logits z\n",
    "y = torch.tensor([1,2]).reshape(-1)\n",
    "np.testing.assert_almost_equal(loss.value(ypred,y), -torch.log(torch.tensor([0.4,0.7])).sum(), decimal=8)\n",
    "np.testing.assert_array_almost_equal(loss.derivative(ypred,y), torch.tensor([[ 0.5000, -0.6000,  0.1000],[ 0.2000,  0.1000, -0.3000]]), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ab724-6516-47ec-9aad-8a2a99a1e78f",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "As in previous' week PW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cb457-1da9-4c14-83c2-345dfe0bf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a76b89-ea2c-4e26-a43d-874ca5b75e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f7fbd-b5fe-432d-b59e-732154aea809",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "Implement mini-batch gradient descent training loop. \n",
    "\n",
    "With the implementation of the two methods below you will be able to train and test the MLP:\n",
    "* train_epoch: for training the model over one epoch with per mini-batch updates\n",
    "* test_epoch: for evaluating the test/validation performance per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d69729-6a94-45a6-b6c6-9ecc7d4f2f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loss, dataloader, lr):\n",
    "    \"\"\"\n",
    "    Iterate over the mini-batches of one epoch, compute per mini-batch the forward and backward pass \n",
    "    and update the parameters. Also compute the loss and accuracy as an average over the epoch. \n",
    "    Note that this average includes per mini-batch updated model predictions and parameter updates.\n",
    "    model -- model to be trained\n",
    "    loss -- loss function to be used \n",
    "    dataloader -- data loader that provides mini-batches (from the training set)\n",
    "    lr -- learning rate to be used in the parameter updates     \n",
    "    returns loss, accuracy \n",
    "    \"\"\"\n",
    "    ### YOUR CODE START ###\n",
    "    nsamples = len(dataloader.dataset)\n",
    "    trainloss, correct = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        batchsize = X.shape[0]\n",
    "        X = X.view(batchsize, -1)\n",
    "        z = model.forward(X)\n",
    "        batchloss = loss.value(z, y)\n",
    "        trainloss += batchloss.item()\n",
    "        correct += (z.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dz = loss.derivative(z,y)\n",
    "        dx = model.backward(dz)\n",
    "        model.update(lr)\n",
    "    trainloss /= nsamples\n",
    "    correct /= nsamples\n",
    "    return trainloss, correct\n",
    "    ### YOUR CODE START ###\n",
    "\n",
    "\n",
    "def test_epoch(model, loss, dataloader):\n",
    "    \"\"\"\n",
    "    Iterate over the mini-batches of one epoch of the test set. Iterates over the mini-batches of the test set.\n",
    "    Estimates loss and accuracy as an average over the test (validation) set. The model is not updates here. \n",
    "    model -- model to be evaluated\n",
    "    loss -- loss function to be evaluated \n",
    "    dataloader -- data loader that provides mini-batches (from the test/validation set)\n",
    "    returns loss, accuracy \n",
    "    \"\"\"\n",
    "    nsamples = len(dataloader.dataset)\n",
    "    testloss, correct = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        batchsize = X.shape[0]\n",
    "        X = X.view(batchsize, -1)\n",
    "        z = model.forward(X)\n",
    "        testloss += loss.value(z, y)\n",
    "        correct += (z.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    testloss /= nsamples\n",
    "    correct /= nsamples\n",
    "    return testloss, correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c7fe4-5c5f-40d4-a7b2-8f650140094f",
   "metadata": {},
   "source": [
    "### First Simple Check: Overfitting on Single Sample\n",
    "\n",
    "Load an arbitrary mini-batch from the training set. Train the model by using just this mini-batch.\n",
    "This is another test for checking whether your implementation is capable of learning something (see remark in week 2 of the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f29756-4bba-473e-83e9-17996fe1b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch, _ = torch.utils.data.random_split(train_data, [64, 60000-64])\n",
    "train_loader = DataLoader(sample_batch, batch_size=64, shuffle=False) # shuffling not needed since only one batch is used.\n",
    "\n",
    "sample_x,sample_y = next(iter(train_loader))\n",
    "print(sample_x.shape,sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c8561-8484-48b2-9369-075f076fa983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1.0\n",
    "mlp = MLP(28*28, [100, 10])\n",
    "mseloss = CELoss()\n",
    "trainlosses = []\n",
    "trainaccs = []\n",
    "for t in range(epochs):\n",
    "    trainloss, trainacc = train_epoch(mlp, mseloss, train_loader, lr)\n",
    "    trainlosses.append(trainloss)\n",
    "    trainaccs.append(trainacc)\n",
    "    if t%10==0:\n",
    "        print(f\"Epoch: {t}, Train Accuracy: {(100*trainacc):>0.1f}%, Train Loss: {trainloss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464995e-355c-4562-8619-7dea93e5d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(torch.arange(epochs), trainlosses,\"b-\")\n",
    "plt.title(\"Train Loss\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(torch.arange(epochs), trainaccs,\"b-\")\n",
    "plt.ylim([0.0,1])\n",
    "plt.title(\"Train Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d366251-a20b-4aac-bc33-a4c09607078c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training with all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f164c3-0e81-478d-ac9e-c5770d55075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2ab4f-b355-4fd3-9fad-7f27d3bd3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1.0\n",
    "mlp = MLP(28*28, [100,10])\n",
    "mseloss = CELoss()\n",
    "trainlosses, testlosses = [],[]\n",
    "trainaccs, testaccs = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trainloss, trainacc = train_epoch(mlp, mseloss, train_loader, lr)\n",
    "    testloss, testacc = test_epoch(mlp, mseloss, test_loader)\n",
    "    trainlosses.append(trainloss)\n",
    "    testlosses.append(testloss)\n",
    "    trainaccs.append(trainacc)\n",
    "    testaccs.append(testacc)\n",
    "    print(f\"Epoch: {epoch}, Train Accuracy: {(100*trainacc):>0.1f}%, Train Loss: {trainloss:>8f}, Test Accuracy: {(100*testacc):>0.1f}%, Test Loss: {testloss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad1661-b9f4-4048-8196-6b76df0a5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(torch.arange(epochs), trainlosses,\"b-\")\n",
    "plt.plot(torch.arange(epochs), testlosses,\"r-\")\n",
    "plt.title(\"CE Loss\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(torch.arange(epochs), trainaccs,\"b-\")\n",
    "plt.plot(torch.arange(epochs), testaccs,\"r-\")\n",
    "plt.ylim([0.7,1])\n",
    "plt.title(\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fec40-f14f-4ef0-b795-bbbee1f4776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
